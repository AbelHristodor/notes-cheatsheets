#cluster-architecture #api-primitives #services 

---
# Introduction
High-level overview of the infrastructure.

## Cluster Architecture
The purpose of k8s is to host applications as containers easily and enable communication between them.
There are two kinds of nodes:
- **worker nodes**: Host applications as containers
- **master nodes**: Manage, plan, schedule and monitor nodes

![[K8s_Architecture_2.png]]

### Master Node (Control Plane)
- **ETCD** is a key-value store (database) containing all the necessary configuration and information about the cluster.
- **Kube Scheduler** identifies the right node to schedule a container based on its resources requirements, and worker node resources, taints, affinities etc..
- **Kube Controller-Manager**:
	- **Node-Controller**: Takes care of nodes, onboarding new nodes, deletes them
	- **Replication-Controller**: Makes sure the required number of instances are running
	- **etc...**
- **Kube APIServer**: Responsible of orchestrating operations in the cluster. Exposes the k8s API and monitors the state of other controller in order to manage the cluster.

- *Container Runtime*: Docker 

### Worker Nodes
- **Kubelet**: agent that runs on each node and listens for instructions from the api-server. 
- **kube-proxy**: service that enables communication between containers

<sub><sup>*end of lesson 11*</sup></sub>

## ETCD
*In general* etcd is a distributed reliable key-value store that is simple, fast and secure.
A *key-value* store stores information in the form of documents or pages written in JSON or YAML
You can download the etcd service and use it as a database. It has a *control client* used it to interact with etcd on port 2379 like `etcdctl set/put key1 value1` or `etcdctl get key1`, command depend on the version of etcd and is api-version.

### ETCD in Kubernetes
It sores information about the cluster for Nodes, Pods, Configs, Secrets etc..
Every change you make to the cluster is stored in the etcd. Only when the change is made in the etcd it is considered as complete and then executed in the cluster.
K8s stores data in a specific directory structure. The *root* is */registry/* and the data is stored in its own directory. E.g. */registry/nodes* etc.

In a HA environment, the etcd instances must know about each other. During the setup of the master etcd it is crucial to set up the other etcd instances.


## Kube API-Server
The commands that kubectl executes reach the apiserver which first authenticates and validates the request. Then it gets the data and returns the response. You could only use the api-server directly.
E.g. you can create a pod by making a post request:
**The API Server is responsible for **
1. *Authentication* - request gets authenticated
2. *Validation* - request gets validated
3. *Retrieve* - data is retrieved, the api creates a pod without assigning it to a *node* and the user is alerted that the pod is being created.
4. *Update Etcd* - the etcd gets updated
5. *Scheduler* - The Scheduler monitors the api-server and realizes there's a new pod. It identifies the appropriate node and tells it to the api-server which updates the information in the etcd. 
6. *Kubelet* - The api-server passes that information to the kubelet which then creates the pod on the node and instructs the container runtime to deploy the defined image. Then the kubelet announces the api-server which updates the data in the etcd.

<small>kube-apiserver is available as a binary in the k8s release page</small>

## Kube Controller Manager
Process that continuously watches the state of the application. It consists of many controllers:
- *Node Controller*: monitor the status of the nodes and tries to remediate the situation, it listens for each nodes' hearthbeat every 5s. Each node has a grace period of 40s
-  ,,,

## Kube Scheduler
Responsible for deciding which pod goes to which node. It goes through two phases in order to decide a node:
- *Filter nodes* - based on resources
- *Rank nodes* - does some calculations in order to understand how many resources would be available on a node after the pod is scheduled on it.

## Kubelet
It registers the node within a k8s cluster and it lives in the worker nodes only. IT is responsible of creating the pods and their monitoring. It is not deployed automatically when installing k8s manually.

## Kube Proxy
Responsible of creating and allowing pods to talk to each other using a Virtual Private Network. It allowes pods to talk to each other using *names* that get *resolved* into IP (which can change). This is done using *services*, it has *no process*, it lives only in memory. The *kube-proxy* listens to new services and creates the proper rules on each node to allow traffic to them using *ip-table rules*. 

# Recaps
Brief recap of the main types of objects available on k8s.

![[deploys_rs.webp]]

## ReplicaSets
In order to have more replicas of a pod you must use a replication controller which ensures that the specified number of pods are running at all times.
It is used also to share the load across more pods in order to achieve high-availability and allow more users to use the app. 
There is a *replication controller* and a *replica set*. Replication controller is an older technology replaced by replica set. 

*Replication Controller:*
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
	name: my-app-rc
	labels:
		app: myapp
		type: backend-rc
spec:
	replicas: 1
	template:
		metadata:
			name: my-app-pod
			labels:
				app: myapp
				type: backend-rc
		spec:
			containers:
				- name: nginx-container
					image: nginx
```

*ReplicaSet:*
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
	name: my-app-rs
	labels:
		app: myapp
		type: backend-rs
spec:
	selector:
		matchLabels: 
			type: backend
	replicas: 1
	template:
		metadata:
			name: my-app-pod
			labels:
				app: myapp
				type: backend-rs
		spec:
			containers:
				- name: nginx-container
					image: nginx
```

The *selector* is used to select the pods that the replicaset has to manage. In fact, it can manage pods that were not created directly by the replicaset

The *replicaset* allows pods to scale using the following command:
```bash
k scale --replicas=6 -f myfile.yaml 
-- OR -- 
k scale replicaset --replicas=6 myreplicasetname
```

## Deployments


## Services
## Namespaces
