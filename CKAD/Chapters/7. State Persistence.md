---
created: 2024-01-11T15:04
updated: 2024-02-19T12:02
Done: true
tags:
  - storage
  - volumes
  - statefulsets
category: Configuration
---
# Volumes
Volumes allow persistence to be added to pods. Without volumes all data would be deleted when the pod gets deleted or restarted.
e.g.
```
...pod definition
spec:
	containers:
	...
	volumes:
	- name: data-volume
	  hostPath:
		  path: /data
		  type: Directory
```
Using the **hostPath** is not recommended since that would be stored on the node itself, so it is better to store data using external services such aws ebs etc..

## Persistent Volume
In order to not create volumes per each pod (e.g. write volume definition on each pod config), we would like to have a large storage pool from which each pod can allocate some space to store its data.

A **Persistent Volume** is a cluster-wide pool of storage volumes to be used by users deploying applications on the cluster.
A user can request a certain amount of space by using **Persistent Volume Claims (PVC)**, after the request gets processed the PVC gets *binded* to the PV.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
	name: pv-vol1
spec:
	accessModes:
		- ReadOnlyOnce
	capacity:
		storage: 1Gi
	hostPath: // Volume type e.g. awsebs etc..
		path: /tmp/data // not recommended in prod env
```

**Available access modes:**
- *ReadOnlyMany*
- *ReadWriteOnce*
- *ReadWriteMany*

### Persistent Volume Claims
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
spec:
	accessModes:
		- ReadWriteOnce
	resources:
		requests:
			storage: 500Mi
```

Use `k get pvc` to retrieve info about the pvc.
If you delete a pvc by default the pv does not get deleted. It can be set by the `persistentVolumeReclaimPolicy` key to be `Recice` or `Retain` to prevent deletion (Default).

## Storage Classes
In order to not manually create the disk each time we need storage for a pv, we can use `storage classes` to automatically provision storage.

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
	name: my-sc
provisioner: kubernetes.io/gce-pod
```

and then define the `storageClassName` in the pvc. Not that you won't need to manually create a PV since the storageclass will be doing that automatically. 
There are also other parameters and configuration that you can pass to the sc but they may depend on the platform used to host your cluster.

## Stateful Sets
StatefulSet is the workload API object used to manage stateful applications.
Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.
*Like a Deployment*, a StatefulSet manages Pods that are based on an identical container spec.* Unlike a Deployment*, a StatefulSet maintains a **sticky identity** for each of its Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling. (e.g. useful with master-slave architecture where you always need to know which pod is the master and which are the slaves).

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "headless-service" # Mandatory
  replicas: 3 # by default is 1
  minReadySeconds: 10 # by default is 0
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: registry.k8s.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
```
The `serviceName` is mandatory. Each pod gets a stable unique DNS record and they get deployed one at the time, only when the previous pod is deployed successfully. 
This can be changed using `podManagementPolicy: Parallel` to deploy all pods concurrently.

But, in a master-slave topology only the master can do writes, and when you deploy a statefulset all pods fall under the same service. How can we separate the master pod from the other pods with services?
We create a **headless service** that points only to the master pod.
```yaml
apiVersion: v1
kind: Service
metadata:
	name: headless-service
spec:
	ports:
	- port: 3303
	selector:
		app: my-app
	clusterIP: None # Important
```
And then in the pod definition
```yaml
...
spec:
	containers:
		..
	subdomain: headless-service
	hostnanme: my_pod_name
```

If you add that pod definition onto a `deployment`, all pods would have the same `my_pod_name` as defined in the deployment config file, so the service will point to the same pod, and this is not what we want.
Statefulset doesn't do that, it automatically assigns `subdomain` and `hostname` accordingly for each pod, but how does it know which *headless service* to use?
Thanks to the `serviceName` field as shown above.

### Storage
If you want for each pod to have its own pvc (and pv) you need to define a `volueclaimtemplate`. Add a volume claim to the *volumeClaimTemplate*.
```yaml
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  minReadySeconds: 10 # by default is 0
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: registry.k8s.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates: # Just a volume template claim
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi
```
