---
created: 2024-01-11T15:04
updated: 2024-02-19T10:32
category: Configuration
Done: true
tags:
  - cronjobs
  - jobs
  - labels
  - selectors
  - deployment
  - strategies
---
# Pod Design

## Labels & Selectors
Labels allow to filter and group items. Selectors allow to *select* those grouped items.

E.g.
```yaml
apiVersion: v1
...
metadata:
	name: app
	labels:
		type: core
		my-label: my-func
```

To select the pod with a specific label use:
`k get pods --selector <my_label>=<my_value>` , for example: `k get pods --selector type=core` to get all pods with the `type=core` label.

This is particularly useful when managing Deployments/ReplicaSets because we can use labels as a replica set selector to group pods under a single deployment/replicaset. E.g.
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
	labels:
		app: my-replicaset-label
spec:
	selector:
		matchLabels:
			app: my-pod-label // This will select all pods with this label
	template:
		metadata:
			labels:
				app: my-pod-label
	containers:
		- ...
```

Note that `app=my-pod-label` label will be added only to the `pods` created by the replicaset (or the existing ones will be selected by it). The `my-replicaset-label` is a label for the `replicaset` only.

This can be applied to any kubernetes object.

## Deployment Rollouts and Versioning
When you first create a deployment kubernetes creates a new `revision`. When you update the deployment, k8s creates a new `revision`. And so on. We can get information about rollouts by using the `k rollout status <my_deployment>` command.
Also this can be seen by using the `describe` command on the deployment.
While, to get info about the history of a deployment we can use the `k rollout  history <deployment>` command.
### Strategies
There are many different deployment strategies.
- **Recreate**: delete all old versions and deploy new ones. 
- **Rolling Update**: update one pod/deployment at the time. One goes down, new one goes up.

`k set image deployment/<my_depl> <new_image>:<tag>` is used to update the image of a deployment. It isn't considered best practice since the file defining the deployment will become outdated. This and any change to the deployment file will cause a rolling update (or any other strategy will be applied to update the deployment).

### Upgrades
When making changes and applying them to a deployment, k8s, under the hood, creates a new replicaset under the same deployment with the new configuration while slowly killing the pods in the old replicaset. 
This behavior can be seen by using `k get replicasets` when applying a new configuration. 

To **undo** a rollout us the `k rollout undo deployment/<my_depl>`.

Here are some handy examples related to updating a Kubernetes Deployment:- **Creating a deployment, checking the rollout status and history:**
In the example below, we will first **create** a simple deployment and inspect the **rollout status** and the **rollout history**:

  
```

1.kubectl create deployment nginx --image=nginx:1.16
 deployment.apps/nginx created

master $ kubectl rollout status deployment nginx
Waiting for deployment "nginx" rollout to finish: 0 of 1 updated replicas are available...
deployment "nginx" successfully rolled out

2.kubectl rollout history deployment nginx
deployment.extensions/nginx
REVISION CHANGE-CAUSE
1     <none>

```

- **Using the --revision flag:**
    

Here the revision 1 is the first version where the deployment was created.

You can check the status of each revision individually by using the **--revision flag**:
```

1. master $ kubectl rollout history deployment nginx --revision=1
2. deployment.extensions/nginx with revision #1

4. Pod Template:
5.  Labels:    app=nginx    pod-template-hash=6454457cdb
6.  Containers:  nginx:  Image:   nginx:1.16
7.   Port:    <none>
8.   Host Port: <none>
9.   Environment:    <none>
10.   Mounts:   <none>
11.  Volumes:   <none>
12. master $ 

  

- **Using the --record flag:**
    

You would have noticed that the "**change-cause**" field is empty in the rollout history output. We can use the **--record flag** to save the command used to create/update a deployment against the revision number.

1. master $ kubectl set image deployment nginx nginx=nginx:1.17 --record
2. deployment.extensions/nginx image updated
3. master $master $

5. master $ kubectl rollout history deployment nginx
6. deployment.extensions/nginx

8. REVISION CHANGE-CAUSE
9. 1     <none>
10. 2     kubectl set image deployment nginx nginx=nginx:1.17 --record=true
11. master $

  

You can now see that the **change-cause** is recorded for the revision 2 of this deployment.

Let's make some more changes. In the example below, we are editing the deployment and changing the image from **nginx:1.17** to **nginx:latest** while making use of the --record flag.

1. master $ kubectl edit deployments. nginx --record
2. deployment.extensions/nginx edited

4. master $ kubectl rollout history deployment nginx
5. REVISION CHANGE-CAUSE
6. 1     <none>
7. 2     kubectl set image deployment nginx nginx=nginx:1.17 --record=true
8. 3     kubectl edit deployments. nginx --record=true

12. master $ kubectl rollout history deployment nginx --revision=3
13. deployment.extensions/nginx with revision #3

15. Pod Template: Labels:    app=nginx
16.     pod-template-hash=df6487dc Annotations: kubernetes.io/change-cause: kubectl edit deployments. nginx --record=true

18.  Containers:
19.   nginx:
20.   Image:   nginx:latest
21.   Port:    <none>
22.   Host Port: <none>
23.   Environment:    <none>
24.   Mounts:   <none>
25.  Volumes:   <none>

27. master $

  

- **Undo a change:**
    

Lets now rollback to the previous revision:

1. controlplane $ kubectl rollout history deployment nginx
2. deployment.apps/nginx 
3. REVISION  CHANGE-CAUSE
4. 1         <none>
5. 3         kubectl edit deployments.apps nginx --record=true
6. 4         kubectl set image deployment nginx nginx=nginx:1.17 --record=true

10. controlplane $ kubectl rollout history deployment nginx --revision=3
11. deployment.apps/nginx with revision #3
12. Pod Template:
13.   Labels:       app=nginx
14.         pod-template-hash=787f54657b
15.   Annotations:  kubernetes.io/change-cause: kubectl edit deployments.apps nginx --record=true
16.   Containers:
17.    nginx:
18.     Image:      nginx:latest
19.     Port:      <none> 
20.     Host Port:  <none>
21.     Environment: <none>       
22.     Mounts:     <none>
23.   Volumes:      

25. controlplane $ kubectl describe deployments. nginx | grep -i image:
26.     Image:        nginx:1.17

28. controlplane $

  

With this, we have rolled back to the previous version of the deployment with the **image = nginx:1.17.**

1. controlplane $ kubectl rollout history deployment nginx --revision=1
2. deployment.apps/nginx with revision #1
3. Pod Template:
4.   Labels:       app=nginx
5.         pod-template-hash=78449c65d4
6.   Containers:
7.    nginx:
8.     Image:      nginx:1.16
9.     Port:       <none> 
10.     Host Port:  <none>
11.     Environment: <none>     
12.     Mounts:     <none>
13.   Volumes:      

15. controlplane $ kubectl rollout undo deployment nginx --to-revision=1
16. deployment.apps/nginx rolled back

To rollback to specific revision we will use the `--to-revision` flag.  
With `--to-revision=1`, it will be rolled back with the first image we used to create a deployment as we can see in the `rollout history` output.

1. controlplane $ kubectl describe deployments. nginx | grep -i image:
2. Image: nginx:1.16
```

## Additional Strategies
### Blue-Green
We have a new version deployed alongside the old one. 100% traffic goes to the old one. Once tests pass for the new version traffic gets redirected to the new deployment. This is usually managed by an external service e.g. **Istio**. But it can also be done using raw kubernetes.
1. Create `blue` deployment (old one) with a service with a label `version: 1` on both the service and deployment
2. Create `green` deployment (new one) with label `version: 2`
3. After tests pass for the `green` deployment change the `labelSelector` on the service to `version: 2`. So, traffic gets redirected to the new deployment.
### Canary
We deploy the new version. A small % of traffic gets redirected to the new version. Once tests pass and everything works, all traffic gets redirected to the new version. This can be done using (but not only) raw kubernetes.
1. Create `primary` deployment with label `version: 1` with a service associated with label `app: frontend`. The deployment also has a new label `app: frontend`.
2. Create `canary` deployment with labels `version: 2, app: frontend`
3. Since the service uses as selector the `app: frontend` label, it will route traffic to both `primary` (50%) and `canary` (50%) deployments. But we want a small percentage to be routed to the canary. So, we scale down the canary deployment to the minimum replicas. E.g. if *primary* has 5 pods, *canary* will have just one and so the percentages will be *83% to 17%*. 
4. Update `primary` and delete `canary` once tests are passed.

This is harder to do with raw k8s, **Istio** allows you to exactly set the percentage of traffic you want to redirect. 

## Jobs
Sometimes we need to run short-lived workloads that might be cpu-intensive or memory intensive and that's it. In order to do that is through Jobs. If we were to use a Pod k8s will try to restart the pod continuously, even if the workload that we wanted to do has already been completed, and that's not what we want. 

In order to achieve that we can use **jobs**. Jobs run a set of pods until completion.
```yaml
apiVersion: batch/v1
kind: Job
metadata:
	name: math-add-jo
spec:
	completions: 3 // sequential number of pods to be created.
	parallelism: 3
// Here we define a pod
	template:
		spec:
			containers:
				- name: math-add
				  image: ubuntu
				  command: ['expr', '3', '+', '2']
			restartPolicy: Never
```
This will create a job that will run until completed. K8s will not restart the pod.
What about the output of the job? We can see that through **logs**. `k get logs <my_job>`

The `completions` fields specifies how many pods there will be with status *completed* at the end.  The job will try to run as many times as it needs to have `completions` number of completed pods, if one fails, it does not get counted.

The `parallelism` field specifies how many pods will be created at a time. If is intelligent enough to understand not to create more pods at the time than the ones needed by the `completions` field.

### Cronjobs
Scheduled job, like crontab.
```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
	name: report-math-add-jo
spec:
	schedule: "*/1 * * * *"
	jobTemplate: // Define a job
		spec:
			completions: 3
			parallelism: 3
			template:
				spec:
					containers:
						- name: math-add
						  image: ubuntu
						  command: ['expr', '3', '+', '2']
					restartPolicy: Never
```