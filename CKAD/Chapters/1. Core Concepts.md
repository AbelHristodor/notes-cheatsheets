---
title: Core Concepts
lessons: 10-37
checkpoint: https://www.udemy.com/course/certified-kubernetes-application-developer/learn/lecture/12299412#overview
tags:
  - ckad
  - kubernetes
created: 2024-01-21T12:25:00
updated: 2024-01-21T14:19
---
# Cluster
Is a set of nodes where, when one node fails, the other nodes take up the load. So, having multiple nodes also helps sharing workload.
## Nodes
Physical machines used to make up a cluster. They can be one of two types:
- **worker nodes**
- **master nodes**
### Worker
Machine, physical or virtual where *kubernetes* is installed. It is a worker machine so it will contain your running application.
If your application goes down, it is better to have more than one node in order to implement zero-downtime or HA (High-Availability) in your cluster.

They have the *kubelet-agent* that is responsible of communicating to the master node and do what the master says.
### Master
Is another node that is responsible of the orchestration of the nodes running on the cluster. It is the only node that has running inside the *kube-apiserver*, *etcd*, *controller*, *API scheduler*.
# Components
When installing k8s on a system, you actually install multiple components:
- API Server
- Etcd
- Kubelet
- Container Runtime
- Controller
- Scheduler

### API Server
Is the frontend for k8s. The users, CLIs such as `kubectl` (See [[Kubectl Cheatsheet]]) etc.. all interact w/ API Server in order to talk to k8s or the application installed in it.

### Etcd
It is a key-value store that holds all necessary information about nodes, master(s) and makes sure, by implementing locks that there are no conflicts between masters when there are more than one.

### Scheduler
It is responsible for distributing work or containers to different nodes. It looks for newly created containers and then it assigns them to different nodes or pods.

### Controller
Is the brain behind orchestration. It is responsible for understanding when nodes/pods/containers go down or need to be created and responds accordingly.

### Container Runtime
Software that allows to run our application in a containerized way. E.g. [Docker](www.docker.com), containerd or others.

### Kubelet
It is the agent running in each node and it is responsible of making sure containers are running as expected on the nodes. Also it communicates with the *kube-apiserver* on the master node sharing other information such as *status*, *health* and other which are all stored on the etcd in the master node.

![[K8s_Architecture_2.png|Kubernetes architecture.]]
<p align="center" style="font-style: italic">Kubernetes general architecture</p>
# Pods
Containers are encapsulated in a k8s object known as *pod*. The smallest object you can create in K8s representing a single instance of your application.

![[Pod.png]]

Pods allow you to scale on the same node or more nodes in order to handle the upcoming traffic. The only way you can scale is by adding more pods and not by duplicating containers in pod. A pod can have multiple containers, usually not by the same kind; sometimes a pod can have a main container and other containers that *help* (known as *helper*) that exists only as long as the main container lives. They usually share the same storage space and are all on the same network. 

#### Pod as Yaml
Pods can be defined as yaml files.
For example:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: nginx-container
      image: nginx:<version|tag>
```

| Key          | Description                                                                         |
| ------------ | ----------------------------------------------------------------------------------- |
| `apiVersion` | Version of the api of reference                                                     |
| `kind`       | Can be of type *Pod, Service, ReplicaSet, Deployment* and other custom resources      |
| `metadata`     | Set metadata values, only keys that k8s expects and nothing else.                   |
| `name`         | Pod name                                                                            |
| `labels`       | Dictionary that can have any key-value pairs. E.g. labels can be used to group pods |
| `spec`         | Specification dictionary containing settings for the pod (or other resources)                                                                                    |

# ReplicaSet
A *replication controller* is used when we want to have more replicas (instances) of the same pod, so if one fails we can still have other instances of our application running. Thus it provides high-availability. 
It is good practice to use a *replication controller* even if you're defining just a single pod with one replica since its *replication controller* ensures that there's always a single pod running, thus, achieving high-availability by restarting the pod when it crashes.

Note: *replication controller* is the older tecnology. Now the new way of implementing replication is achieved by using the **ReplicaSet**. All above is still valid for the newer technology.

In order to let know what pod to *replicate*, we must define a `template` into the replica controller for it to know how the pod replicated should look like.

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
	name: myapp-rc
	labels: 
		app: myapp
		type: frontend
spec:
	template:
		metadata:
			name: myapp-pod
			labels:
				app: myapp
				type: frontend
		spec:
			containers:
				- name: nginx
				  image: nginx
	replicas: 3
```

To manage it you can retrieve the object: `k get replicationcontroller`

Let's now check the **ReplicaSet**
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
	name: myapp-rc
	labels: 
		app: myapp
		type: frontend
spec:
	template:
		metadata:
			name: myapp-pod
			labels:
				app: myapp
				type: frontend
		spec:
			containers:
				- name: nginx
				  image: nginx
	replicas: 3
	selector:
		matchLabels:
			type: frontend
```
As you can see the main difference is the presence of the `selector` key. The *selector* keys allows us to define what pods should fall into the *replicaset* domain i.e. what pods the **rs** should have control over. This is because a **rs** can control pods that weren't created by the **rs** but have the correct labels. E.g. you can create a *rs* that monitors already pre-created pods. The `template` definition section is *mandatory* because, in case one of the pre-created pods dies, the *rs* must have a way of recreating it.

To manage it you can retrieve the object: `k get replicaset`

### Labels & Selectors
Why do we need to have labels and selectors in k8s?
In order to understand what *pods* a *replicaset* should monitor, it needs someway of *finding* and *filtering* them. We use `labels` to group pods and so *select* them using the *selector*.

### Scaling
In order to *increase* or *decrease* the number of pods (*replicas*) of a **replicaset**, there are two ways: 
- `declarative`way where you change the file definition of the rs and then apply it
- `imperative` by using this command `k scale --replicas=<REPLICAS> replicaset <NAME>`. This won't change the file definition.

You can also make *rs* scale based on load. That's called *horizontal scaling*. 

# Deployment


**Next:** [[2. Configuration]]